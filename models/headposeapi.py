# -*- coding: utf-8 -*-
"""HeadPoseAPI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yAXOUjvqqe1z1-kfqiL0Y2oKSy8haTdn
"""

!unzip /media/archive -d /media

!ls /media/AFLW2000

!pip install scipy

import scipy.io
import os

# Define the full path to your dataset folder.
# Based on your `ls` output, the path is /media/AFLW2000
dataset_path = '/media/AFLW2000'

# List all files in the directory to find a .mat file
files_in_dir = os.listdir(dataset_path)

# Find the first .mat file in the list
mat_file = None
for file in files_in_dir:
    if file.endswith('.mat'):
        mat_file = file
        break

if mat_file:
    # Construct the full path to the .mat file
    file_path = os.path.join(dataset_path, mat_file)

    # Load the data from the .mat file
    mat_data = scipy.io.loadmat(file_path)

    print(f"Successfully loaded data from: {file_path}")
    print("Keys available in the .mat file:")
    print(mat_data.keys())

    # Example: Accessing a variable. You'll need to replace 'landmarks'
    # with the actual key name from the output above.
    try:
        # Assuming one of the keys is 'landmarks_3d'
        landmarks_3d = mat_data['pt3d_68']
        print(f"\nShape of the landmarks data: {landmarks_3d.shape}")
        print("A sample of the landmark data:")
        print(landmarks_3d[:3])  # Print the first 3 rows of data
    except KeyError:
        print("\n'landmarks_3d' key not found. Please check the keys printed above and use the correct one.")
else:
    print("No .mat files found in the directory.")

import numpy as np

# Set the path to the directory containing your files
dataset_path = '/media/AFLW2000'

# Get a list of all .mat files in the directory
mat_files = [f for f in os.listdir(dataset_path) if f.endswith('.mat')]

print(f"Found {len(mat_files)} .mat files to process.")

# Create an empty list to store the results
all_neck_angles_deg = []

# Loop through each .mat file
for file_name in mat_files:
    # Construct the full path
    file_path = os.path.join(dataset_path, file_name)

    try:
        # Load the data from the .mat file
        mat_data = scipy.io.loadmat(file_path)

        # Access the Pose_Para key
        pose_parameters = mat_data['Pose_Para']

        # Extract the first three values (pitch, yaw, roll)
        neck_angles_rad = pose_parameters[0, :3]

        # Convert from radians to degrees
        neck_angles_deg = np.degrees(neck_angles_rad)

        # Append the results to our list
        all_neck_angles_deg.append(neck_angles_deg)

        # Optional: Print the results for each file
        print(f"Processed {file_name}: Pitch={neck_angles_deg[0]:.2f}, Yaw={neck_angles_deg[1]:.2f}, Roll={neck_angles_deg[2]:.2f} (degrees)")

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

# Convert the list to a single NumPy array for easy manipulation
all_neck_angles_deg_array = np.array(all_neck_angles_deg)

print("\n--- Processing Complete ---")
print(f"Successfully processed {len(all_neck_angles_deg_array)} files.")
print(f"Shape of the final results array: {all_neck_angles_deg_array.shape}")

import numpy as np

# Assuming all_neck_angles_deg_array is already populated from the previous step
# It should have a shape of (num_images, 3)

# Save the array to a CSV file. The delimiter is a comma.
np.savetxt('neck_angles.csv', all_neck_angles_deg_array, delimiter=',')

print("Successfully saved neck angles to 'neck_angles.csv'")

!pip install mediapipe

import os
import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import scipy.io

# Define paths
dataset_path = '/media/AFLW2000'
output_angles_csv = 'neck_angles.csv'
output_landmarks_csv = 'facial_landmarks.csv'

# --- 1. Load Ground Truth Neck Angles from .mat files ---
print("Extracting neck angles from .mat files...")
mat_files = [f for f in os.listdir(dataset_path) if f.endswith('.mat')]
all_neck_angles_deg = []
file_names = []

for file_name in mat_files:
    file_path = os.path.join(dataset_path, file_name)
    try:
        mat_data = scipy.io.loadmat(file_path)
        pose_parameters = mat_data['Pose_Para']
        neck_angles_rad = pose_parameters[0, :3]
        neck_angles_deg = np.degrees(neck_angles_rad)
        all_neck_angles_deg.append(neck_angles_deg)
        file_names.append(os.path.splitext(file_name)[0] + '.jpg')
    except Exception as e:
        print(f"Error processing {file_name}: {e}")

all_neck_angles_deg_array = np.array(all_neck_angles_deg)
angles_df = pd.DataFrame(all_neck_angles_deg_array, columns=['pitch', 'yaw', 'roll'])
angles_df['image_name'] = file_names

# Save the angles
angles_df.to_csv(output_angles_csv, index=False)
print(f"‚úÖ Saved {len(angles_df)} neck angles to {output_angles_csv}")

# --- 2. MediaPipe Feature Extraction with Preprocessing ---
print("\nExtracting and preprocessing landmarks with MediaPipe...")
mp_face_mesh = mp.solutions.face_mesh
image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith('.jpg')]
landmark_data = []

# The landmark index for the nose tip is a crucial fixed point for centering
NOSE_TIP_LANDMARK_IDX = 1

def preprocess_landmarks(face_landmarks):
    """
    Centers landmarks on the nose tip and normalizes coordinates.
    Returns a flattened NumPy array.
    """
    # MediaPipe returns 478 landmarks
    # Extract x and y coordinates
    x_coords = np.array([lm.x for lm in face_landmarks.landmark])
    y_coords = np.array([lm.y for lm in face_landmarks.landmark])

    # Get the coordinates of the nose tip landmark (index 1)
    nose_x = x_coords[NOSE_TIP_LANDMARK_IDX]
    nose_y = y_coords[NOSE_TIP_LANDMARK_IDX]

    # Center the landmarks by subtracting the nose coordinates
    x_centered = x_coords - nose_x
    y_centered = y_coords - nose_y

    # Normalize the centered coordinates by the max absolute value
    max_val = max(np.abs(x_centered).max(), np.abs(y_centered).max())
    x_normalized = x_centered / max_val if max_val != 0 else x_centered
    y_normalized = y_centered / max_val if max_val != 0 else y_centered

    return np.concatenate([x_normalized, y_normalized])

with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1) as face_mesh:
    for img_name in image_files:
        img_path = os.path.join(dataset_path, img_name)
        bgr = cv2.imread(img_path)
        if bgr is None: continue
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

        results = face_mesh.process(rgb)
        if results.multi_face_landmarks:
            face = results.multi_face_landmarks[0]
            preprocessed_coords = preprocess_landmarks(face)
            landmark_data.append([img_name] + preprocessed_coords.tolist())

# Corrected column creation logic
if landmark_data:
    # Get the number of landmark points from the first processed data entry
    num_landmarks = (len(landmark_data[0]) - 1) // 2

    # Create column names: 'image_name', 'x_0', 'y_0', 'x_1', 'y_1', etc.
    cols = ['image_name']
    for i in range(num_landmarks):
        cols.append(f'x_{i}')
        cols.append(f'y_{i}')

    df_landmarks = pd.DataFrame(landmark_data, columns=cols)
    df_landmarks.to_csv(output_landmarks_csv, index=False)
    print(f'‚úÖ Saved preprocessed landmarks to {output_landmarks_csv}')
else:
    print('‚ö†Ô∏è No landmarks detected')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
import joblib

# Load the prepared data
df_landmarks = pd.read_csv('facial_landmarks.csv')
angles_df = pd.read_csv('neck_angles.csv')

# Merge the dataframes on the image name to ensure correct alignment
merged = pd.merge(df_landmarks, angles_df, on='image_name', how='inner')

# Explicitly drop the non-feature columns ('image_name' and the three angle columns)
X = merged.drop(['image_name', 'pitch', 'yaw', 'roll'], axis=1).values
y = merged[['pitch', 'yaw', 'roll']].values

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the models
models = {
    'Pitch_SVR': SVR(kernel='rbf'),
    'Yaw_SVR': SVR(kernel='rbf'),
    'Roll_SVR': SVR(kernel='rbf'),
    'Pitch_RF': RandomForestRegressor(n_estimators=100, random_state=42),
    'Yaw_RF': RandomForestRegressor(n_estimators=100, random_state=42),
    'Roll_RF': RandomForestRegressor(n_estimators=100, random_state=42)
}

for angle, model in models.items():
    print(f"Training {angle} model...")
    if 'Pitch' in angle:
        y_train_angle = y_train[:, 0]
    elif 'Yaw' in angle:
        y_train_angle = y_train[:, 1]
    else:
        y_train_angle = y_train[:, 2]

    model.fit(X_train, y_train_angle)
    joblib.dump(model, f'{angle}_model.joblib')
    print(f"Model for {angle} saved.")

import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import joblib
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from math import sin, cos, radians
import os
import tempfile
from IPython.display import display, Image
from google.colab.patches import cv2_imshow # ÿ£ÿ∂ŸÅ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ∑ÿ± ÿ•ÿ∞ÿß ŸÉŸÜÿ™ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ Colab

# --- 1. Load Data and Prepare for Evaluation ---
df_landmarks = pd.read_csv('facial_landmarks.csv')
angles_df = pd.read_csv('neck_angles.csv')
merged = pd.merge(df_landmarks, angles_df, on='image_name', how='inner')
X = merged.drop(['image_name', 'pitch', 'yaw', 'roll'], axis=1).values
y = merged[['pitch', 'yaw', 'roll']].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 2. Load all trained models ---
print("Loading trained models...")
models_all = {
    'Pitch_SVR': joblib.load('Pitch_SVR_model.joblib'),
    'Yaw_SVR': joblib.load('Yaw_SVR_model.joblib'),
    'Roll_SVR': joblib.load('Roll_RF_model.joblib'),
    'Pitch_RF': joblib.load('Pitch_RF_model.joblib'),
    'Yaw_RF': joblib.load('Yaw_RF_model.joblib'),
    'Roll_RF': joblib.load('Roll_RF_model.joblib')
}

# --- 3. Evaluate all models and choose the best for each angle ---
best_models = {}
for angle in ['Pitch', 'Yaw', 'Roll']:
    angle_idx = ['Pitch', 'Yaw', 'Roll'].index(angle)
    candidates = {}

    for k in models_all:
        if angle in k:
            y_pred = models_all[k].predict(X_test)
            mse = mean_squared_error(y_test[:, angle_idx], y_pred)
            candidates[k] = mse

    best_name = min(candidates, key=candidates.get)
    best_models[angle] = models_all[best_name]
    print(f"‚úÖ Best {angle} model: {best_name} (MSE={candidates[best_name]:.4f})")

print("--- Evaluation Complete ---")
print("Proceeding with prediction and visualization.")

# --- 4. Function to Draw Axes ---
def draw_axis_on_image(img, pitch, yaw, roll, nose_coords, size=100):
    yaw *= -1
    R_x = np.array([[1, 0, 0], [0, cos(radians(pitch)), -sin(radians(pitch))], [0, sin(radians(pitch)), cos(radians(pitch))]])
    R_y = np.array([[cos(radians(yaw)), 0, sin(radians(yaw))], [0, 1, 0], [-sin(radians(yaw)), 0, cos(radians(yaw))]])
    R_z = np.array([[cos(radians(roll)), -sin(radians(roll)), 0], [sin(radians(roll)), cos(radians(roll)), 0], [0, 0, 1]])
    R = R_y @ R_x @ R_z
    x_axis_end = R @ np.array([size, 0, 0])
    y_axis_end = R @ np.array([0, -size, 0])
    z_axis_end = R @ np.array([0, 0, -size])
    x_origin, y_origin = int(nose_coords[0]), int(nose_coords[1])
    cv2.arrowedLine(img, (x_origin, y_origin), (int(x_origin + x_axis_end[0]), int(y_origin + x_axis_end[1])), (0, 0, 255), 3)
    cv2.arrowedLine(img, (x_origin, y_origin), (int(x_origin + y_axis_end[0]), int(y_origin + y_axis_end[1])), (0, 255, 0), 3)
    cv2.arrowedLine(img, (x_origin, y_origin), (int(x_origin + z_axis_end[0]), int(y_origin + z_axis_end[1])), (255, 0, 0), 2)
    return img

# --- 5. Function for Full Prediction Pipeline ---
def predict_and_visualize(image_path, best_models, preprocess_func):
    mp_face_mesh = mp.solutions.face_mesh
    img = cv2.imread(image_path)
    if img is None:
        raise FileNotFoundError(f"Can't read image: {image_path}")
    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1) as face_mesh:
        results = face_mesh.process(rgb)
        if not results.multi_face_landmarks:
            print("‚ö†Ô∏è No face detected.")
            return None, img

        face_landmarks = results.multi_face_landmarks[0]
        nose_lm = face_landmarks.landmark[1]
        nose_coords = (nose_lm.x * img.shape[1], nose_lm.y * img.shape[0])

        preprocessed_coords = preprocess_func(face_landmarks)
        coords_array = preprocessed_coords.reshape(1, -1)

        pitch = best_models['Pitch'].predict(coords_array)[0]
        yaw = best_models['Yaw'].predict(coords_array)[0]
        roll = best_models['Roll'].predict(coords_array)[0]

        annotated_img = img.copy()

        # ŸÑÿß Ÿäÿ™ŸÖ ÿ±ÿ≥ŸÖ ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿÆÿ∂ÿ±ÿßÿ°

        annotated_img = draw_axis_on_image(annotated_img, pitch, yaw, roll, nose_coords)

        return [pitch, yaw, roll], annotated_img

# --- Example of external testing ---
try:
    X = merged.drop(['image_name', 'pitch', 'yaw', 'roll'], axis=1).values

    def preprocess_landmarks(face_landmarks):
        x_coords = np.array([lm.x for lm in face_landmarks.landmark])
        y_coords = np.array([lm.y for lm in face_landmarks.landmark])
        nose_x = x_coords[1]
        nose_y = y_coords[1]
        x_centered = x_coords - nose_x
        y_centered = y_coords - nose_y
        max_val = max(np.abs(x_centered).max(), np.abs(y_centered).max())
        x_normalized = x_centered / max_val if max_val != 0 else x_centered
        y_normalized = y_centered / max_val if max_val != 0 else y_centered
        return np.concatenate([x_normalized, y_normalized])

    angles_pred, annotated_img = predict_and_visualize(
        '/media/test/images.jpeg', best_models, preprocess_landmarks)

    if angles_pred is not None:
        # üü¢ ÿ™ÿπÿØŸäŸÑ ÿ≥ÿ∑ÿ± ÿßŸÑÿ∑ÿ®ÿßÿπÿ© ŸÑÿ•ÿ∂ÿßŸÅÿ© ŸàÿµŸÅ ÿßŸÑÿ£ŸÑŸàÿßŸÜ
        print("Predicted angles (in degrees):")
        print(f"  Pitch (Red): {angles_pred[0]:.2f}¬∞")
        print(f"  Yaw (Green): {angles_pred[1]:.2f}¬∞")
        print(f"  Roll (Blue): {angles_pred[2]:.2f}¬∞")

        cv2_imshow(annotated_img) # ÿßÿ≥ÿ™ÿÆÿØŸÖ cv2_imshow ŸÅŸä Colab
    else:
        print("No image to display.")

except FileNotFoundError as e:
    print(e)
except Exception as e:
    print(f"An error occurred: {e}")

import cv2
import mediapipe as mp
import numpy as np
import joblib
from math import sin, cos, radians
from google.colab import files
from IPython.display import HTML, clear_output
from base64 import b64encode

# --- 1. Load trained models ---
try:
    print("Loading trained models...")
    best_models = {
        'Pitch': joblib.load('Pitch_SVR_model.joblib'),
        'Yaw': joblib.load('Yaw_RF_model.joblib'),
        'Roll': joblib.load('Roll_RF_model.joblib')
    }
    print("‚úÖ Models loaded successfully.")
except FileNotFoundError as e:
    print(f"Error: Could not load model files. Please ensure they are in the correct directory. {e}")
    exit()
except Exception as e:
    print(f"An error occurred while loading models: {e}")
    exit()

# --- 2. Function to Draw Axes ---
def draw_axis_on_image(img, pitch, yaw, roll, nose_coords, size=100):
    yaw *= -1
    R_x = np.array([[1, 0, 0], [0, cos(radians(pitch)), -sin(radians(pitch))], [0, sin(radians(pitch)), cos(radians(pitch))]])
    R_y = np.array([[cos(radians(yaw)), 0, sin(radians(yaw))], [0, 1, 0], [-sin(radians(yaw)), 0, cos(radians(yaw))]])
    R_z = np.array([[cos(radians(roll)), -sin(radians(roll)), 0], [sin(radians(roll)), cos(radians(roll)), 0], [0, 0, 1]])
    R = R_y @ R_x @ R_z
    x_axis_end = R @ np.array([size, 0, 0])
    y_axis_end = R @ np.array([0, -size, 0])
    # üü¢ ÿ™ÿπÿØŸäŸÑ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ∑ÿ± ŸÑÿπŸÉÿ≥ ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑÿ£ÿ≤ÿ±ŸÇ
    z_axis_end = R @ np.array([0, 0, size])

    x_origin, y_origin = int(nose_coords[0]), int(nose_coords[1])

    # ÿßŸÑŸÖÿ≠ÿßŸàÿ± ÿ®ÿßŸÑÿ£ŸÑŸàÿßŸÜ BGR:
    # ÿßŸÑÿ£ÿ≠ŸÖÿ±: (0, 0, 255)
    # ÿßŸÑÿ£ÿÆÿ∂ÿ±: (0, 255, 0)
    # ÿßŸÑÿ£ÿ≤ÿ±ŸÇ: (255, 0, 0)

    cv2.arrowedLine(img, (x_origin, y_origin), (int(x_origin + x_axis_end[0]), int(y_origin + x_axis_end[1])), (0, 0, 255), 3)
    cv2.arrowedLine(img, (x_origin, y_origin), (int(x_origin + y_axis_end[0]), int(y_origin + y_axis_end[1])), (0, 255, 0), 3)
    cv2.arrowedLine(img, (x_origin, y_origin), (int(x_origin + z_axis_end[0]), int(y_origin + z_axis_end[1])), (255, 0, 0), 2)
    return img

# --- 3. Function to Preprocess Landmarks ---
def preprocess_landmarks(face_landmarks):
    x_coords = np.array([lm.x for lm in face_landmarks.landmark])
    y_coords = np.array([lm.y for lm in face_landmarks.landmark])
    nose_x = x_coords[1]
    nose_y = y_coords[1]
    x_centered = x_coords - nose_x
    y_centered = y_coords - nose_y
    max_val = max(np.abs(x_centered).max(), np.abs(y_centered).max())
    x_normalized = x_centered / max_val if max_val != 0 else x_centered
    y_normalized = y_centered / max_val if max_val != 0 else y_centered
    return np.concatenate([x_normalized, y_normalized])

# --- 4. Main Video Processing Function ---
def process_and_save_video(video_source, best_models):
    mp_face_mesh = mp.solutions.face_mesh
    cap = cv2.VideoCapture(video_source)

    if not cap.isOpened():
        print(f"Error: Could not open video source at {video_source}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    output_filename = 'output_video.mp4'

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))

    print("Starting video processing. This may take some time.")

    with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1) as face_mesh:
        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)

            annotated_frame = frame.copy()
            angles_pred = None

            if results.multi_face_landmarks:
                face_landmarks = results.multi_face_landmarks[0]
                nose_lm = face_landmarks.landmark[1]
                nose_coords = (nose_lm.x * frame.shape[1], nose_lm.y * frame.shape[0])

                preprocessed_coords = preprocess_landmarks(face_landmarks)
                coords_array = preprocessed_coords.reshape(1, -1)

                pitch = best_models['Pitch'].predict(coords_array)[0]
                yaw = best_models['Yaw'].predict(coords_array)[0]
                roll = best_models['Roll'].predict(coords_array)[0]

                angles_pred = [pitch, yaw, roll]
                annotated_frame = draw_axis_on_image(annotated_frame, pitch, yaw, roll, nose_coords)

            if angles_pred is not None:
                print(f"Frame {frame_count}: Pitch: {angles_pred[0]:.2f}¬∞, Yaw: {angles_pred[1]:.2f}¬∞, Roll: {angles_pred[2]:.2f}¬∞")
            else:
                 print(f"Frame {frame_count}: No face detected.")

            video_writer.write(annotated_frame)
            frame_count += 1

    cap.release()
    video_writer.release()
    print("Video processing finished.")

    mp4 = open(output_filename, 'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
    display(HTML("""
    <video width=600 controls>
        <source src="%s" type="video/mp4">
    </video>
    """ % data_url))
    print(f"Video saved as '{output_filename}'")
    files.download(output_filename)


# --- 5. Run the script ---
process_and_save_video("/media/test/video_man.webm", best_models)

import cv2
import mediapipe as mp
import numpy as np
import joblib
from math import sin, cos, radians
from google.colab import files
from IPython.display import HTML, display
from base64 import b64encode

# --- 1. Load trained models ---
try:
    print("Loading trained models...")
    best_models = {
        'Pitch': joblib.load('Pitch_SVR_model.joblib'),
        'Yaw': joblib.load('Yaw_RF_model.joblib'),
        'Roll': joblib.load('Roll_RF_model.joblib')
    }
    print("‚úÖ Models loaded successfully.")
except Exception as e:
    print(f"Error loading models: {e}")
    raise

# --- 2. Functions (Defined once) ---

def draw_axis_on_image(img, pitch, yaw, roll, nose_coords, size=100, z_positive=True):
    """Draw 3D axes on the image at the nose location."""
    yaw *= -1
    R_x = np.array([[1, 0, 0],
                    [0, cos(radians(pitch)), -sin(radians(pitch))],
                    [0, sin(radians(pitch)), cos(radians(pitch))]])
    R_y = np.array([[cos(radians(yaw)), 0, sin(radians(yaw))],
                    [0, 1, 0],
                    [-sin(radians(yaw)), 0, cos(radians(yaw))]])
    R_z = np.array([[cos(radians(roll)), -sin(radians(roll)), 0],
                    [sin(radians(roll)), cos(radians(roll)), 0],
                    [0, 0, 1]])
    R = R_y @ R_x @ R_z

    x_axis_end = R @ np.array([size, 0, 0])
    y_axis_end = R @ np.array([0, -size, 0])
    z_axis_end = R @ np.array([0, 0, size if z_positive else -size])

    x_origin, y_origin = int(nose_coords[0]), int(nose_coords[1])

    cv2.arrowedLine(img, (x_origin, y_origin),
                    (int(x_origin + x_axis_end[0]), int(y_origin + x_axis_end[1])), (0, 0, 255), 3)
    cv2.arrowedLine(img, (x_origin, y_origin),
                    (int(x_origin + y_axis_end[0]), int(y_origin + y_axis_end[1])), (0, 255, 0), 3)
    cv2.arrowedLine(img, (x_origin, y_origin),
                    (int(x_origin + z_axis_end[0]), int(y_origin + z_axis_end[1])), (255, 0, 0), 2)
    return img

def preprocess_landmarks(face_landmarks):
    """Center landmarks on the nose tip and normalize."""
    x_coords = np.array([lm.x for lm in face_landmarks.landmark])
    y_coords = np.array([lm.y for lm in face_landmarks.landmark])
    nose_x, nose_y = x_coords[1], y_coords[1]
    x_centered = x_coords - nose_x
    y_centered = y_coords - nose_y
    max_val = max(np.abs(x_centered).max(), np.abs(y_centered).max())
    x_normalized = x_centered / max_val if max_val != 0 else x_centered
    y_normalized = y_centered / max_val if max_val != 0 else y_centered
    return np.concatenate([x_normalized, y_normalized])

# --- 3. Predict on single image ---
def predict_image(image_path):
    mp_face_mesh = mp.solutions.face_mesh
    img = cv2.imread(image_path)
    if img is None:
        raise FileNotFoundError(f"Cannot read image: {image_path}")
    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1) as face_mesh:
        results = face_mesh.process(rgb)
        if not results.multi_face_landmarks:
            print("‚ö†Ô∏è No face detected.")
            return None, img

        face_landmarks = results.multi_face_landmarks[0]
        nose_lm = face_landmarks.landmark[1]
        nose_coords = (nose_lm.x * img.shape[1], nose_lm.y * img.shape[0])

        coords_array = preprocess_landmarks(face_landmarks).reshape(1, -1)

        pitch = best_models['Pitch'].predict(coords_array)[0]
        yaw = best_models['Yaw'].predict(coords_array)[0]
        roll = best_models['Roll'].predict(coords_array)[0]

        annotated_img = draw_axis_on_image(img.copy(), pitch, yaw, roll, nose_coords, z_positive=False)
        return [pitch, yaw, roll], annotated_img

# --- 4. Process video ---
def process_video(video_source):
    mp_face_mesh = mp.solutions.face_mesh
    cap = cv2.VideoCapture(video_source)
    if not cap.isOpened():
        print(f"Cannot open video: {video_source}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    width, height = int(cap.get(3)), int(cap.get(4))
    output_filename = 'output_video.mp4'
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))

    with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1) as face_mesh:
        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)

            annotated_frame = frame.copy()
            angles_pred = None
            if results.multi_face_landmarks:
                face_landmarks = results.multi_face_landmarks[0]
                nose_lm = face_landmarks.landmark[1]
                nose_coords = (nose_lm.x * frame.shape[1], nose_lm.y * frame.shape[0])
                coords_array = preprocess_landmarks(face_landmarks).reshape(1, -1)
                pitch = best_models['Pitch'].predict(coords_array)[0]
                yaw = best_models['Yaw'].predict(coords_array)[0]
                roll = best_models['Roll'].predict(coords_array)[0]
                angles_pred = [pitch, yaw, roll]
                annotated_frame = draw_axis_on_image(annotated_frame, pitch, yaw, roll, nose_coords, z_positive=True)

            if angles_pred:
                print(f"Frame {frame_count}: Pitch {angles_pred[0]:.2f}, Yaw {angles_pred[1]:.2f}, Roll {angles_pred[2]:.2f}")
            frame_count += 1
            writer.write(annotated_frame)

    cap.release()
    writer.release()
    print("‚úÖ Video saved:", output_filename)

    mp4 = open(output_filename,'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
    display(HTML(f'<video width=600 controls><source src="{data_url}" type="video/mp4"></video>'))
    files.download(output_filename)

# --- 5. Usage examples ---
# For image:
# angles, img_out = predict_image("/media/test/images.jpeg")
# cv2.imshow("Annotated", img_out)

# For video:
# process_video("/media/test/video_man.webm")

#--- 5. Usage examples ---
#For image:
angles, img_out = predict_image("/media/test/images.jpeg")
# Use cv2_imshow instead of cv2.imshow in Colab
cv2_imshow(img_out)

# For video:
process_video("/media/test/video_man.webm")

